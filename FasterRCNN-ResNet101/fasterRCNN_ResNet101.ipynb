{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(1, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, re\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "MODELS_OUT_DIR = CONFIG.CFG.DATA.MODELS_OUT\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ON_CPU = DEVICE == torch.device('cpu')\n",
    "TRAIN_BATCH_SIZE = 4 if ON_CPU else 6\n",
    "VALID_BATCH_SIZE = 2 if ON_CPU else 2\n",
    "NUM_EPOCHS = 5 if ON_CPU else 7\n",
    "NEW_COLUMNS = ['x', 'y', 'w', 'h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((147793, 5), (10, 2))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "UNIQ_TRAIN_IMAGE_IDS = train_df[\"image_id\"].unique()\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>222.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>226.0</td>\n      <td>548.0</td>\n      <td>130.0</td>\n      <td>58.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>377.0</td>\n      <td>504.0</td>\n      <td>74.0</td>\n      <td>160.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>95.0</td>\n      <td>109.0</td>\n      <td>107.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>26.0</td>\n      <td>144.0</td>\n      <td>124.0</td>\n      <td>117.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    image_id  width  height   source      x      y      w      h\n0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# expand the bbox coordinates into x, y, w, h\n",
    "def expand_bbox(x):\n",
    "    # also convert everything to np.float\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x), dtype=np.float)\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "# initialize new columns with -1\n",
    "for new_column in NEW_COLUMNS:\n",
    "    train_df[new_column] = -1\n",
    "\n",
    "train_df[NEW_COLUMNS] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(boxes, image, color=(255,0,0)):\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 3\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_train_sample():\n",
    "    image_id = np.random.choice(UNIQ_TRAIN_IMAGE_IDS)\n",
    "    plt.title(image_id)\n",
    "    image = load_image(os.path.join(DATA_DIR, \"train\", f\"{image_id}.jpg\"))\n",
    "    bboxes = (train_df[train_df[\"image_id\"] == image_id][NEW_COLUMNS]).to_numpy()\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    plt.imshow(draw_bboxes(bboxes, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_train_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = df['image_id'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = load_image(os.path.join(self.image_dir, f\"{image_id}.jpg\")).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        # change the shape from [h,w,c] to [c,h,w]  \n",
    "        image = torch.from_numpy(image).permute(2,0,1)\n",
    "\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        boxes = records[NEW_COLUMNS].values\n",
    "        area = boxes[:, 2] * boxes[:, 3]\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # change the co-ordinates into expected [x, y, x+w, y+h] format\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # since all the boxes are wheat, it's all 1s\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # consider iscrowd false for all the boxes\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            pass\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    We change the backbone to use the ResNet101 model instead, DETAILS AT:\n",
    "    https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#modifying-the-model-to-add-a-different-backbone\n",
    "    \"\"\"\n",
    "    resnet_101 = torchvision.models.resnet101(pretrained=True)\n",
    "    layers = list(resnet_101.children())[:-2]\n",
    "    backbone = torch.nn.Sequential(*layers)\n",
    "    backbone.out_channels = 2048\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "    # roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "    #                                             output_size=7,\n",
    "    #                                             sampling_ratio=2)\n",
    "\n",
    "    # put the pieces together inside a FasterRCNN model\n",
    "    model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3036 training samples\n337 validation samples\n"
    }
   ],
   "source": [
    "# 10% into validation\n",
    "n_validation = int(0.1* len(UNIQ_TRAIN_IMAGE_IDS))\n",
    "valid_ids = UNIQ_TRAIN_IMAGE_IDS[-n_validation:]\n",
    "train_ids = UNIQ_TRAIN_IMAGE_IDS[:-n_validation]\n",
    "\n",
    "df_in_valid = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "df_in_train = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(\"%i training samples\\n%i validation samples\" % (len(df_in_train[\"image_id\"].unique()), len(df_in_valid[\"image_id\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WheatDataset(df_in_train, os.path.join(DATA_DIR, \"train\"))\n",
    "valid_dataset = WheatDataset(df_in_valid, os.path.join(DATA_DIR, \"train\"))\n",
    "\n",
    "# since our single getitem returns image, targets. [shape of targets is different depending on the number of bounding boxes in the image] ?\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check the batch is working\n",
    "# batch_of_images, batch_of_targets = next(iter(train_data_loader))\n",
    "# sample_boxes = batch_of_targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
    "# # convert to normal image format\n",
    "# sample_image = batch_of_images[0].permute(1,2,0).cpu().numpy()\n",
    "# plt.imshow(draw_bboxes(sample_boxes, sample_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n\tnonzero(Tensor input, *, Tensor out)\nConsider using one of the following signatures instead:\n\tnonzero(Tensor input, *, bool as_tuple)\nEpoch: 0 Batch Index: 0 Loss: 1.7282410860061646\nEpoch: 0 Batch Index: 50 Loss: 1.3839623928070068\nEpoch: 0 Batch Index: 100 Loss: 1.1567453145980835\nEpoch: 0 Batch Index: 150 Loss: 0.880188524723053\nEpoch: 0 Batch Index: 200 Loss: 0.759335458278656\nEpoch: 0 Batch Index: 250 Loss: 0.8168365955352783\nEpoch: 0 Batch Index: 300 Loss: 0.588411808013916\nEpoch: 0 Batch Index: 350 Loss: 0.950764000415802\nEpoch: 0 Batch Index: 400 Loss: 0.682932436466217\nEpoch: 0 Batch Index: 450 Loss: 0.6813760995864868\nEpoch: 0 Batch Index: 500 Loss: 0.8028496503829956\nEpoch #0 TRAIN LOSS: 0.8869344017251207 VALIDATION LOSS: 0.6858514231690288\n\nEpoch: 1 Batch Index: 0 Loss: 0.7109065651893616\nEpoch: 1 Batch Index: 50 Loss: 0.5002651214599609\nEpoch: 1 Batch Index: 100 Loss: 0.5744882822036743\nEpoch: 1 Batch Index: 150 Loss: 0.6444576978683472\nEpoch: 1 Batch Index: 200 Loss: 0.7050467133522034\nEpoch: 1 Batch Index: 250 Loss: 0.6847798824310303\nEpoch: 1 Batch Index: 300 Loss: 0.6402013301849365\nEpoch: 1 Batch Index: 350 Loss: 0.5851672887802124\nEpoch: 1 Batch Index: 400 Loss: 0.6280434131622314\nEpoch: 1 Batch Index: 450 Loss: 0.6553356051445007\nEpoch: 1 Batch Index: 500 Loss: 0.6092215776443481\nEpoch #1 TRAIN LOSS: 0.6351322789672806 VALIDATION LOSS: 0.6272847782930678\n\nEpoch: 2 Batch Index: 0 Loss: 0.5566134452819824\nEpoch: 2 Batch Index: 50 Loss: 0.674098014831543\nEpoch: 2 Batch Index: 100 Loss: 0.5934118628501892\nEpoch: 2 Batch Index: 150 Loss: 0.5484706163406372\nEpoch: 2 Batch Index: 200 Loss: 0.5437778234481812\nEpoch: 2 Batch Index: 250 Loss: 0.5714096426963806\nEpoch: 2 Batch Index: 300 Loss: 0.5532881617546082\nEpoch: 2 Batch Index: 350 Loss: 0.6100770235061646\nEpoch: 2 Batch Index: 400 Loss: 0.5158300399780273\nEpoch: 2 Batch Index: 450 Loss: 0.5282154679298401\nEpoch: 2 Batch Index: 500 Loss: 0.5449429154396057\nEpoch #2 TRAIN LOSS: 0.5645184599952735 VALIDATION LOSS: 0.6027225183664694\n\nEpoch: 3 Batch Index: 0 Loss: 0.4793836772441864\nEpoch: 3 Batch Index: 50 Loss: 0.6027513146400452\nEpoch: 3 Batch Index: 100 Loss: 0.5526507496833801\nEpoch: 3 Batch Index: 150 Loss: 0.40764737129211426\nEpoch: 3 Batch Index: 200 Loss: 0.5254127979278564\nEpoch: 3 Batch Index: 250 Loss: 0.430900514125824\nEpoch: 3 Batch Index: 300 Loss: 0.5970271825790405\nEpoch: 3 Batch Index: 350 Loss: 0.5191394686698914\nEpoch: 3 Batch Index: 400 Loss: 0.5630518198013306\nEpoch: 3 Batch Index: 450 Loss: 0.5152715444564819\nEpoch: 3 Batch Index: 500 Loss: 0.5254839062690735\nEpoch #3 TRAIN LOSS: 0.5255262857604875 VALIDATION LOSS: 0.616082516824\n\nEpoch: 4 Batch Index: 0 Loss: 0.525814950466156\nEpoch: 4 Batch Index: 50 Loss: 0.47042715549468994\nEpoch: 4 Batch Index: 100 Loss: 0.46916818618774414\nEpoch: 4 Batch Index: 150 Loss: 0.5262988209724426\nEpoch: 4 Batch Index: 200 Loss: 0.5053482055664062\nEpoch: 4 Batch Index: 250 Loss: 0.45406287908554077\nEpoch: 4 Batch Index: 300 Loss: 0.43965524435043335\nEpoch: 4 Batch Index: 350 Loss: 0.5093085765838623\nEpoch: 4 Batch Index: 400 Loss: 0.5659525394439697\nEpoch: 4 Batch Index: 450 Loss: 0.5158659219741821\nEpoch: 4 Batch Index: 500 Loss: 0.4368014931678772\nEpoch #4 TRAIN LOSS: 0.4935206843341292 VALIDATION LOSS: 0.6144813038188325\n\nEpoch: 5 Batch Index: 0 Loss: 0.43732398748397827\nEpoch: 5 Batch Index: 50 Loss: 0.44941478967666626\nEpoch: 5 Batch Index: 100 Loss: 0.4358800947666168\nEpoch: 5 Batch Index: 150 Loss: 0.460026353597641\nEpoch: 5 Batch Index: 200 Loss: 0.4907614290714264\nEpoch: 5 Batch Index: 250 Loss: 0.4560384750366211\nEpoch: 5 Batch Index: 300 Loss: 0.5010578632354736\nEpoch: 5 Batch Index: 350 Loss: 0.5593977570533752\nEpoch: 5 Batch Index: 400 Loss: 0.41741517186164856\nEpoch: 5 Batch Index: 450 Loss: 0.39439815282821655\nEpoch: 5 Batch Index: 500 Loss: 0.5020982027053833\nEpoch #5 TRAIN LOSS: 0.470929007402993 VALIDATION LOSS: 0.591296559783834\n\nEpoch: 6 Batch Index: 0 Loss: 0.4487307071685791\nEpoch: 6 Batch Index: 50 Loss: 0.45564714074134827\nEpoch: 6 Batch Index: 100 Loss: 0.4043121039867401\nEpoch: 6 Batch Index: 150 Loss: 0.41404905915260315\nEpoch: 6 Batch Index: 200 Loss: 0.4908645749092102\nEpoch: 6 Batch Index: 250 Loss: 0.5332611799240112\nEpoch: 6 Batch Index: 300 Loss: 0.40935081243515015\nEpoch: 6 Batch Index: 350 Loss: 0.3531363904476166\nEpoch: 6 Batch Index: 400 Loss: 0.4101073741912842\nEpoch: 6 Batch Index: 450 Loss: 0.5429654121398926\nEpoch: 6 Batch Index: 500 Loss: 0.42579415440559387\nEpoch #6 TRAIN LOSS: 0.44850193152550183 VALIDATION LOSS: 0.6035705769555808\n\n"
    }
   ],
   "source": [
    "train_losses = Averager()\n",
    "val_losses = Averager()\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# has to be in train mode for both train and valid coz the outputs are different in two cases\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_losses.reset()\n",
    "    val_losses.reset()\n",
    "\n",
    "    for batch_index, (images, targets) in enumerate(train_data_loader):\n",
    "        # move the images and targets to device\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images,targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # track the loss\n",
    "        train_losses.send(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {batch_index} Loss: {loss.item()}\")\n",
    "\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(valid_data_loader):\n",
    "            # move the images and targets to device\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_loss = sum(loss for loss in val_loss_dict.values())\n",
    "\n",
    "            # track the loss\n",
    "            val_losses.send(val_loss.item())\n",
    "\n",
    "    if val_losses.value < lowest_val_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, \"best_model.pth\"))\n",
    "    else:\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    \n",
    "    # print stats\n",
    "    print(f\"Epoch #{epoch} TRAIN LOSS: {train_losses.value} VALIDATION LOSS: {val_losses.value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, 'fasterRCNN_101.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"fasterrcnnresnet101\", \"Faster R-CNN ResNet-101\", new=Fal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}