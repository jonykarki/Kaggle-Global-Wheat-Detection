{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(1, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "MODELS_OUT_DIR = CONFIG.CFG.DATA.MODELS_OUT\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ON_CPU = DEVICE == torch.device('cpu')\n",
    "TRAIN_BATCH_SIZE = 4 if ON_CPU else 6\n",
    "VALID_BATCH_SIZE = 2 if ON_CPU else 2\n",
    "NUM_EPOCHS = 5 if ON_CPU else 7\n",
    "NEW_COLUMNS = ['x', 'y', 'w', 'h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "UNIQ_TRAIN_IMAGE_IDS = train_df[\"image_id\"].unique()\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the bbox coordinates into x, y, w, h\n",
    "def expand_bbox(x):\n",
    "    # also convert everything to np.float\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x), dtype=np.float)\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "# initialize new columns with -1\n",
    "for new_column in NEW_COLUMNS:\n",
    "    train_df[new_column] = -1\n",
    "\n",
    "train_df[NEW_COLUMNS] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(boxes, image, color=(255,0,0)):\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 3\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_train_sample():\n",
    "    image_id = np.random.choice(UNIQ_TRAIN_IMAGE_IDS)\n",
    "    plt.title(image_id)\n",
    "    image = load_image(os.path.join(DATA_DIR, \"train\", f\"{image_id}.jpg\"))\n",
    "    bboxes = (train_df[train_df[\"image_id\"] == image_id][NEW_COLUMNS]).to_numpy()\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    plt.imshow(draw_bboxes(bboxes, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_train_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "        self.image_ids = df['image_id'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        p_ratio = random.random()\n",
    "        \n",
    "        if self.test or p_ratio > 0.6:\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "        else:\n",
    "            if p_ratio < 0.2:\n",
    "                image, boxes = self.load_mosaic_image_and_boxes(index)\n",
    "            elif p_ratio < 0.4:\n",
    "                image, boxes = self.load_image_and_bboxes_with_cutmix(index)\n",
    "            else:\n",
    "                image, boxes = self.load_mixup_image_and_boxes(index)\n",
    "        \n",
    "        # change the shape from [h,w,c] to [c,h,w]  \n",
    "        # image = torch.from_numpy(image).permute(2,0,1)\n",
    "\n",
    "        # change the co-ordinates into expected [x, y, x+w, y+h] format\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # since all the boxes are wheat, it's all 1s\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([index])\n",
    "\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            while len(aug['bboxes']) != labels.shape[0]:\n",
    "                aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = aug['image']\n",
    "            target['boxes'] = torch.as_tensor(aug['bboxes'], dtype=torch.float)\n",
    "            target['labels'] = torch.as_tensor(aug['labels'], dtype=torch.int64)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = load_image(os.path.join(self.image_dir, f\"{image_id}.jpg\")).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes\n",
    "\n",
    "    def load_mosaic_image_and_boxes(self, index, imsize=1024):\n",
    "        \"\"\"\n",
    "        This implementation of mosaic author:  https://www.kaggle.com/nvnnghia\n",
    "        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n",
    "        \"\"\"\n",
    "        w, h = imsize, imsize\n",
    "        s = imsize // 2\n",
    "\n",
    "        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n",
    "        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n",
    "\n",
    "        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n",
    "        result_boxes = []\n",
    "\n",
    "        for i, index in enumerate(indexes):\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "            if i == 0:\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            padw = x1a - x1b\n",
    "            padh = y1a - y1b\n",
    "\n",
    "            boxes[:, 0] += padw\n",
    "            boxes[:, 1] += padh\n",
    "            boxes[:, 2] += padw\n",
    "            boxes[:, 3] += padh\n",
    "\n",
    "            result_boxes.append(boxes)\n",
    "\n",
    "        result_boxes = np.concatenate(result_boxes, 0)\n",
    "        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n",
    "        result_boxes = result_boxes.astype(np.int32)\n",
    "        result_boxes = result_boxes[\n",
    "            np.where((result_boxes[:, 2] - result_boxes[:, 0]) * (result_boxes[:, 3] - result_boxes[:, 1]) > 0)]\n",
    "        return result_image, result_boxes\n",
    "\n",
    "    def load_image_and_bboxes_with_cutmix(self, index):\n",
    "        image, bboxes = self.load_image_and_boxes(index)\n",
    "        image_to_be_mixed, bboxes_to_be_mixed = self.load_image_and_boxes(\n",
    "            random.randint(0, self.image_ids.shape[0] - 1))\n",
    "\n",
    "        image_size = image.shape[0]\n",
    "        cutoff_x1, cutoff_y1 = [int(random.uniform(image_size * 0.0, image_size * 0.49)) for _ in range(2)]\n",
    "        cutoff_x2, cutoff_y2 = [int(random.uniform(image_size * 0.5, image_size * 1.0)) for _ in range(2)]\n",
    "\n",
    "        image_cutmix = image.copy()\n",
    "        image_cutmix[cutoff_y1:cutoff_y2, cutoff_x1:cutoff_x2] = image_to_be_mixed[cutoff_y1:cutoff_y2,\n",
    "                                                                 cutoff_x1:cutoff_x2]\n",
    "\n",
    "        # Begin preparing bboxes_cutmix.\n",
    "        # Case 1. Bounding boxes not intersect with cut off patch.\n",
    "        bboxes_not_intersect = bboxes[np.concatenate((np.where(bboxes[:, 0] > cutoff_x2),\n",
    "                                                      np.where(bboxes[:, 2] < cutoff_x1),\n",
    "                                                      np.where(bboxes[:, 1] > cutoff_y2),\n",
    "                                                      np.where(bboxes[:, 3] < cutoff_y1)), axis=None)]\n",
    "\n",
    "        # Case 2. Bounding boxes intersect with cut off patch.\n",
    "        bboxes_intersect = bboxes.copy()\n",
    "\n",
    "        top_intersect = np.where((bboxes[:, 0] < cutoff_x2) &\n",
    "                                 (bboxes[:, 2] > cutoff_x1) &\n",
    "                                 (bboxes[:, 1] < cutoff_y2) &\n",
    "                                 (bboxes[:, 3] > cutoff_y2))\n",
    "        right_intersect = np.where((bboxes[:, 0] < cutoff_x2) &\n",
    "                                   (bboxes[:, 2] > cutoff_x2) &\n",
    "                                   (bboxes[:, 1] < cutoff_y2) &\n",
    "                                   (bboxes[:, 3] > cutoff_y1))\n",
    "        bottom_intersect = np.where((bboxes[:, 0] < cutoff_x2) &\n",
    "                                    (bboxes[:, 2] > cutoff_x1) &\n",
    "                                    (bboxes[:, 1] < cutoff_y1) &\n",
    "                                    (bboxes[:, 3] > cutoff_y1))\n",
    "        left_intersect = np.where((bboxes[:, 0] < cutoff_x1) &\n",
    "                                  (bboxes[:, 2] > cutoff_x1) &\n",
    "                                  (bboxes[:, 1] < cutoff_y2) &\n",
    "                                  (bboxes[:, 3] > cutoff_y1))\n",
    "\n",
    "        # Remove redundant indices. e.g. a bbox which intersects in both right and top.\n",
    "        right_intersect = np.setdiff1d(right_intersect, top_intersect)\n",
    "        right_intersect = np.setdiff1d(right_intersect, bottom_intersect)\n",
    "        right_intersect = np.setdiff1d(right_intersect, left_intersect)\n",
    "        bottom_intersect = np.setdiff1d(bottom_intersect, top_intersect)\n",
    "        bottom_intersect = np.setdiff1d(bottom_intersect, left_intersect)\n",
    "        left_intersect = np.setdiff1d(left_intersect, top_intersect)\n",
    "\n",
    "        bboxes_intersect[:, 1][top_intersect] = cutoff_y2\n",
    "        bboxes_intersect[:, 0][right_intersect] = cutoff_x2\n",
    "        bboxes_intersect[:, 3][bottom_intersect] = cutoff_y1\n",
    "        bboxes_intersect[:, 2][left_intersect] = cutoff_x1\n",
    "\n",
    "        bboxes_intersect[:, 1][top_intersect] = cutoff_y2\n",
    "        bboxes_intersect[:, 0][right_intersect] = cutoff_x2\n",
    "        bboxes_intersect[:, 3][bottom_intersect] = cutoff_y1\n",
    "        bboxes_intersect[:, 2][left_intersect] = cutoff_x1\n",
    "\n",
    "        bboxes_intersect = bboxes_intersect[np.concatenate((top_intersect,\n",
    "                                                            right_intersect,\n",
    "                                                            bottom_intersect,\n",
    "                                                            left_intersect), axis=None)]\n",
    "\n",
    "        # Case 3. Bounding boxes inside cut off patch.\n",
    "        bboxes_to_be_mixed[:, [0, 2]] = bboxes_to_be_mixed[:, [0, 2]].clip(min=cutoff_x1, max=cutoff_x2)\n",
    "        bboxes_to_be_mixed[:, [1, 3]] = bboxes_to_be_mixed[:, [1, 3]].clip(min=cutoff_y1, max=cutoff_y2)\n",
    "\n",
    "        # Integrate all those three cases.\n",
    "        bboxes_cutmix = np.vstack((bboxes_not_intersect, bboxes_intersect, bboxes_to_be_mixed)).astype(int)\n",
    "        bboxes_cutmix = bboxes_cutmix[np.where((bboxes_cutmix[:, 2] - bboxes_cutmix[:, 0]) \\\n",
    "                                               * (bboxes_cutmix[:, 3] - bboxes_cutmix[:, 1]) > 500)]\n",
    "        # End preparing bboxes_cutmix.\n",
    "\n",
    "        return image_cutmix, bboxes_cutmix\n",
    "\n",
    "    def load_mixup_image_and_boxes(self, index):\n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "        r_image, r_boxes = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n",
    "        return (image + r_image) / 2, np.vstack((boxes, r_boxes)).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "#     \"\"\"\n",
    "#     We change the backbone to use the ResNet101 model instead, DETAILS AT:\n",
    "#     https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#modifying-the-model-to-add-a-different-backbone\n",
    "#     \"\"\"\n",
    "#     resnet_101 = torchvision.models.resnet101(pretrained=True)\n",
    "#     layers = list(resnet_101.children())[:-2]\n",
    "#     backbone = torch.nn.Sequential(*layers)\n",
    "#     backbone.out_channels = 2048\n",
    "\n",
    "#     anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "#     # roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "#     #                                             output_size=7,\n",
    "#     #                                             sampling_ratio=2)\n",
    "\n",
    "#     # put the pieces together inside a FasterRCNN model\n",
    "#     model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/58362892/resnet-18-as-backbone-in-faster-r-cnn\n",
    "    \"\"\"\n",
    "    backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "    model = FasterRCNN(backbone, num_classes=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3036 training samples\n337 validation samples\n"
    }
   ],
   "source": [
    "# 10% into validation\n",
    "n_validation = int(0.1* len(UNIQ_TRAIN_IMAGE_IDS))\n",
    "valid_ids = UNIQ_TRAIN_IMAGE_IDS[-n_validation:]\n",
    "train_ids = UNIQ_TRAIN_IMAGE_IDS[:-n_validation]\n",
    "\n",
    "df_in_valid = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "df_in_train = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(\"%i training samples\\n%i validation samples\" % (len(df_in_train[\"image_id\"].unique()), len(df_in_valid[\"image_id\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WheatDataset(df_in_train, os.path.join(DATA_DIR, \"train\"), get_train_transforms())\n",
    "valid_dataset = WheatDataset(df_in_valid, os.path.join(DATA_DIR, \"train\"), get_valid_transforms())\n",
    "\n",
    "# since our single getitem returns image, targets. [shape of targets is different depending on the number of bounding boxes in the image] ?\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check the batch is working\n",
    "# batch_of_images, batch_of_targets = next(iter(train_data_loader))\n",
    "# sample_boxes = batch_of_targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
    "# # convert to normal image format\n",
    "# sample_image = batch_of_images[0].permute(1,2,0).cpu().numpy()\n",
    "# plt.imshow(draw_bboxes(sample_boxes, sample_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13415fb046f4e1093ab3536f640a3db",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\nEpoch: 0 Batch Index: 0 Loss: 2.056020736694336\nEpoch: 0 Batch Index: 50 Loss: 1.7711585760116577\nEpoch: 0 Batch Index: 100 Loss: 1.54567551612854\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 0 Batch Index: 150 Loss: 1.61532723903656\nEpoch: 0 Batch Index: 200 Loss: 1.1056830883026123\nEpoch: 0 Batch Index: 250 Loss: 1.2715328931808472\nEpoch: 0 Batch Index: 300 Loss: 1.1010257005691528\nEpoch: 0 Batch Index: 350 Loss: 1.1631100177764893\nEpoch: 0 Batch Index: 400 Loss: 1.2094600200653076\nEpoch: 0 Batch Index: 450 Loss: 0.9699239730834961\nEpoch: 0 Batch Index: 500 Loss: 1.0728538036346436\nEpoch #0 TRAIN LOSS: 1.311361580380338 VALIDATION LOSS: 1.0238746411701631\n\nEpoch: 1 Batch Index: 0 Loss: 1.0286777019500732\nEpoch: 1 Batch Index: 50 Loss: 0.9090320467948914\nEpoch: 1 Batch Index: 100 Loss: 1.133758544921875\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 1 Batch Index: 150 Loss: 0.9068431854248047\nEpoch: 1 Batch Index: 200 Loss: 1.0403635501861572\nEpoch: 1 Batch Index: 250 Loss: 0.9128440618515015\nEpoch: 1 Batch Index: 300 Loss: 0.8894364833831787\nEpoch: 1 Batch Index: 350 Loss: 0.8073445558547974\nEpoch: 1 Batch Index: 400 Loss: 0.9629306793212891\nEpoch: 1 Batch Index: 450 Loss: 0.9593527317047119\nEpoch: 1 Batch Index: 500 Loss: 0.9880661964416504\nEpoch #1 TRAIN LOSS: 0.9886002806806753 VALIDATION LOSS: 0.9141966617671695\n\nEpoch: 2 Batch Index: 0 Loss: 1.004734992980957\nEpoch: 2 Batch Index: 50 Loss: 0.7782825231552124\nEpoch: 2 Batch Index: 100 Loss: 0.8642809987068176\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 2 Batch Index: 150 Loss: 0.9257997274398804\nEpoch: 2 Batch Index: 200 Loss: 0.9976534843444824\nEpoch: 2 Batch Index: 250 Loss: 0.87059086561203\nEpoch: 2 Batch Index: 300 Loss: 0.8808754086494446\nEpoch: 2 Batch Index: 350 Loss: 0.9014697074890137\nEpoch: 2 Batch Index: 400 Loss: 0.9812702536582947\nEpoch: 2 Batch Index: 450 Loss: 0.9146986603736877\nEpoch: 2 Batch Index: 500 Loss: 0.9795386791229248\nEpoch #2 TRAIN LOSS: 0.9090099598579255 VALIDATION LOSS: 0.8948147004172646\n\nEpoch: 3 Batch Index: 0 Loss: 0.9453839063644409\nEpoch: 3 Batch Index: 50 Loss: 0.7106001377105713\nEpoch: 3 Batch Index: 100 Loss: 0.866871178150177\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 3 Batch Index: 150 Loss: 0.808660626411438\nEpoch: 3 Batch Index: 200 Loss: 0.823017954826355\nEpoch: 3 Batch Index: 250 Loss: 0.9548643231391907\nEpoch: 3 Batch Index: 300 Loss: 0.8707114458084106\nEpoch: 3 Batch Index: 350 Loss: 0.8475847244262695\nEpoch: 3 Batch Index: 400 Loss: 0.8648208379745483\nEpoch: 3 Batch Index: 450 Loss: 0.8211007118225098\nEpoch: 3 Batch Index: 500 Loss: 0.7797452211380005\nEpoch #3 TRAIN LOSS: 0.8675039561369674 VALIDATION LOSS: 0.8349700083394023\n\nEpoch: 4 Batch Index: 0 Loss: 0.8284100890159607\nEpoch: 4 Batch Index: 50 Loss: 0.9219956398010254\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 4 Batch Index: 100 Loss: 0.835462212562561\nEpoch: 4 Batch Index: 150 Loss: 0.9103143215179443\nEpoch: 4 Batch Index: 200 Loss: 0.794082522392273\nEpoch: 4 Batch Index: 250 Loss: 0.9033132195472717\nEpoch: 4 Batch Index: 300 Loss: 0.9147162437438965\nEpoch: 4 Batch Index: 350 Loss: 1.0819242000579834\nEpoch: 4 Batch Index: 400 Loss: 0.9406130909919739\nEpoch: 4 Batch Index: 450 Loss: 0.8232166767120361\nEpoch: 4 Batch Index: 500 Loss: 0.74309241771698\nEpoch #4 TRAIN LOSS: 0.8459346340343415 VALIDATION LOSS: 0.8329143684643966\n\nEpoch: 5 Batch Index: 0 Loss: 0.6878848075866699\nEpoch: 5 Batch Index: 50 Loss: 0.9212763905525208\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 5 Batch Index: 100 Loss: 0.8424575328826904\nEpoch: 5 Batch Index: 150 Loss: 0.7665406465530396\nEpoch: 5 Batch Index: 200 Loss: 0.7172607183456421\nEpoch: 5 Batch Index: 250 Loss: 0.7991403341293335\nEpoch: 5 Batch Index: 300 Loss: 0.9524766206741333\nEpoch: 5 Batch Index: 350 Loss: 0.7057046890258789\nEpoch: 5 Batch Index: 400 Loss: 0.8753541111946106\nEpoch: 5 Batch Index: 450 Loss: 0.8520064353942871\nEpoch: 5 Batch Index: 500 Loss: 0.7285292148590088\nEpoch #5 TRAIN LOSS: 0.8219753350664976 VALIDATION LOSS: 0.8588086867473534\n\nEpoch: 6 Batch Index: 0 Loss: 0.7116525769233704\nEpoch: 6 Batch Index: 50 Loss: 0.8537890911102295\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2938d0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\nAssertionError: can only join a child process\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nException ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f5d6c2936a0>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nEpoch: 6 Batch Index: 100 Loss: 0.7596186399459839\nEpoch: 6 Batch Index: 150 Loss: 0.8257343769073486\nEpoch: 6 Batch Index: 200 Loss: 0.7188631892204285\nEpoch: 6 Batch Index: 250 Loss: 0.7196429967880249\nEpoch: 6 Batch Index: 300 Loss: 0.8006571531295776\nEpoch: 6 Batch Index: 350 Loss: 0.8166701793670654\nEpoch: 6 Batch Index: 400 Loss: 0.698522686958313\nEpoch: 6 Batch Index: 450 Loss: 0.9197095632553101\nEpoch: 6 Batch Index: 500 Loss: 0.7945067286491394\nEpoch #6 TRAIN LOSS: 0.8162007768870342 VALIDATION LOSS: 0.8592010307946855\n\n"
    }
   ],
   "source": [
    "train_losses = Averager()\n",
    "val_losses = Averager()\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# has to be in train mode for both train and valid coz the outputs are different in two cases\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_losses.reset()\n",
    "    val_losses.reset()\n",
    "\n",
    "    for batch_index, (images, targets) in enumerate(train_data_loader):\n",
    "        # move the images and targets to device\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images,targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # track the loss\n",
    "        train_losses.send(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {batch_index} Loss: {loss.item()}\")\n",
    "\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(valid_data_loader):\n",
    "            # move the images and targets to device\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_loss = sum(loss for loss in val_loss_dict.values())\n",
    "\n",
    "            # track the loss\n",
    "            val_losses.send(val_loss.item())\n",
    "\n",
    "    if val_losses.value < lowest_val_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, \"best_model.pth\"))\n",
    "    else:\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    \n",
    "    # print stats\n",
    "    print(f\"Epoch #{epoch} TRAIN LOSS: {train_losses.value} VALIDATION LOSS: {val_losses.value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, 'fasterRCNN_101.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"fasterrcnnresnet101\", \"Faster R-CNN ResNet-101\", new=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10, 2)"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# ####\n",
    "# INFERENCE\n",
    "# ####\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "#         # change the shape from [h,w,c] to [c,h,w]  \n",
    "#         image = torch.from_numpy(image).permute(2,0,1)\n",
    "\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "    \n",
    "        if self.transforms:\n",
    "            sample = {\"image\": image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WheatDataset(test_df, os.path.join(DATA_DIR, \"test\"), get_test_transforms())\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
    }
   ],
   "source": [
    "detection_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "for images, image_ids in test_data_loader:\n",
    "\n",
    "    images = list(image.to(DEVICE) for image in images)\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{'PredictionString': '0.9908 36 0 49 82 0.9858 274 29 72 98 0.9849 350 198 55 81 0.9818 282 269 57 90 0.9811 309 461 36 47 0.9777 125 43 62 73 0.9688 181 267 45 37 0.9641 169 332 51 74 0.9628 231 429 38 44 0.9589 298 391 41 54 0.9534 371 388 36 50 0.9451 38 430 54 36 0.9423 121 421 50 39 0.9351 15 228 42 68 0.9310 48 308 54 45 0.9214 162 434 36 42 0.8923 411 353 46 99 0.8543 91 287 50 90 0.7773 154 0 33 31 0.6144 415 314 37 61 0.6081 418 456 47 55',\n  'image_id': 'aac893a91'},\n {'PredictionString': '0.9848 307 45 76 86 0.9714 139 235 66 62 0.9528 329 395 49 38 0.9202 34 348 57 101 0.9015 256 235 91 51 0.8988 407 382 45 41 0.8814 333 290 48 44 0.8784 352 463 35 35 0.8621 422 134 61 96 0.8350 288 302 48 53 0.8247 105 469 42 41 0.8091 15 0 40 33 0.7987 0 190 25 51 0.7942 414 47 47 38 0.7622 385 444 65 38 0.7578 170 70 45 93 0.7080 127 62 36 55 0.6637 456 336 47 42 0.5518 187 77 33 74',\n  'image_id': '51f1be19e'}]"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "results[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df.head()\n",
    "test_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}