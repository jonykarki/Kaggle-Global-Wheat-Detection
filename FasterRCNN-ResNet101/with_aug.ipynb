{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(1, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "MODELS_OUT_DIR = CONFIG.CFG.DATA.MODELS_OUT\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ON_CPU = DEVICE == torch.device('cpu')\n",
    "TRAIN_BATCH_SIZE = 4 if ON_CPU else 6\n",
    "VALID_BATCH_SIZE = 2 if ON_CPU else 2\n",
    "NUM_EPOCHS = 5 if ON_CPU else 7\n",
    "NEW_COLUMNS = ['x', 'y', 'w', 'h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "UNIQ_TRAIN_IMAGE_IDS = train_df[\"image_id\"].unique()\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the bbox coordinates into x, y, w, h\n",
    "def expand_bbox(x):\n",
    "    # also convert everything to np.float\n",
    "    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x), dtype=np.float)\n",
    "    if len(r) == 0:\n",
    "        r = [-1, -1, -1, -1]\n",
    "    return r\n",
    "\n",
    "# initialize new columns with -1\n",
    "for new_column in NEW_COLUMNS:\n",
    "    train_df[new_column] = -1\n",
    "\n",
    "train_df[NEW_COLUMNS] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(boxes, image, color=(255,0,0)):\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 3\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_train_sample():\n",
    "    image_id = np.random.choice(UNIQ_TRAIN_IMAGE_IDS)\n",
    "    plt.title(image_id)\n",
    "    image = load_image(os.path.join(DATA_DIR, \"train\", f\"{image_id}.jpg\"))\n",
    "    bboxes = (train_df[train_df[\"image_id\"] == image_id][NEW_COLUMNS]).to_numpy()\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    plt.imshow(draw_bboxes(bboxes, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_train_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "        self.image_ids = df['image_id'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        p_ratio = random.random()\n",
    "        \n",
    "        if self.test or p_ratio > 0.6:\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "        else:\n",
    "            if p_ratio < 0.2:\n",
    "                image, boxes = self.load_mosaic_image_and_boxes(index)\n",
    "            elif p_ratio < 0.4:\n",
    "                image, boxes = self.load_image_and_bboxes_with_cutmix(index)\n",
    "            else:\n",
    "                image, boxes = self.load_mixup_image_and_boxes(index)\n",
    "        \n",
    "        # change the shape from [h,w,c] to [c,h,w]  \n",
    "        # image = torch.from_numpy(image).permute(2,0,1)\n",
    "\n",
    "        # change the co-ordinates into expected [x, y, x+w, y+h] format\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # since all the boxes are wheat, it's all 1s\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            while len(aug['boxes']) != labels.shape[0]:\n",
    "                aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = aug['image']\n",
    "            target['boxes'] = aug['bboxes']\n",
    "            target['labels'] = aug['labels']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = load_image(os.path.join(self.image_dir, f\"{image_id}.jpg\")).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes\n",
    "\n",
    "    def load_mosaic_image_and_boxes(self, index, imsize=1024):\n",
    "        \"\"\"\n",
    "        This implementation of mosaic author:  https://www.kaggle.com/nvnnghia\n",
    "        Refactoring and adaptation: https://www.kaggle.com/shonenkov\n",
    "        \"\"\"\n",
    "        w, h = imsize, imsize\n",
    "        s = imsize // 2\n",
    "\n",
    "        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n",
    "        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n",
    "\n",
    "        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n",
    "        result_boxes = []\n",
    "\n",
    "        for i, index in enumerate(indexes):\n",
    "            image, boxes = self.load_image_and_boxes(index)\n",
    "            if i == 0:\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
    "            elif i == 1:  # top right\n",
    "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
    "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
    "            elif i == 2:  # bottom left\n",
    "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n",
    "            elif i == 3:  # bottom right\n",
    "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
    "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
    "            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "            padw = x1a - x1b\n",
    "            padh = y1a - y1b\n",
    "\n",
    "            boxes[:, 0] += padw\n",
    "            boxes[:, 1] += padh\n",
    "            boxes[:, 2] += padw\n",
    "            boxes[:, 3] += padh\n",
    "\n",
    "            result_boxes.append(boxes)\n",
    "\n",
    "        result_boxes = np.concatenate(result_boxes, 0)\n",
    "        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n",
    "        result_boxes = result_boxes.astype(np.int32)\n",
    "        result_boxes = result_boxes[\n",
    "            np.where((result_boxes[:, 2] - result_boxes[:, 0]) * (result_boxes[:, 3] - result_boxes[:, 1]) > 0)]\n",
    "        return result_image, result_boxes\n",
    "\n",
    "    def load_image_and_bboxes_with_cutmix(self, index):\n",
    "        image, bboxes = self.load_image_and_boxes(index)\n",
    "        image_to_be_mixed, bboxes_to_be_mixed = self.load_image_and_boxes(\n",
    "            random.randint(0, self.image_ids.shape[0] - 1))\n",
    "\n",
    "        image_size = image.shape[0]\n",
    "        cutoff_x1, cutoff_y1 = [int(random.uniform(image_size * 0.0, image_size * 0.49)) for _ in range(2)]\n",
    "        cutoff_x2, cutoff_y2 = [int(random.uniform(image_size * 0.5, image_size * 1.0)) for _ in range(2)]\n",
    "\n",
    "        image_cutmix = image.copy()\n",
    "        image_cutmix[cutoff_y1:cutoff_y2, cutoff_x1:cutoff_x2] = image_to_be_mixed[cutoff_y1:cutoff_y2,\n",
    "                                                                 cutoff_x1:cutoff_x2]\n",
    "\n",
    "        # Begin preparing bboxes_cutmix.\n",
    "        # Case 1. Bounding boxes not intersect with cut off patch.\n",
    "        bboxes_not_intersect = bboxes[np.concatenate((np.where(bboxes[:, 0] > cutoff_x2),\n",
    "                                                      np.where(bboxes[:, 2] < cutoff_x1),\n",
    "                                                      np.where(bboxes[:, 1] > cutoff_y2),\n",
    "                                                      np.where(bboxes[:, 3] < cutoff_y1)), axis=None)]\n",
    "\n",
    "        # Case 2. Bounding boxes intersect with cut off patch.\n",
    "        bboxes_intersect = bboxes.copy()\n",
    "\n",
    "        top_intersect = np.where((bboxes[:, 0] < cutoff_x2) &\n",
    "                                 (bboxes[:, 2] > cutoff_x1) &\n",
    "                                 (bboxes[:, 1] < cutoff_y2) &\n",
    "                                 (bboxes[:, 3] > cutoff_y2))\n",
    "        right_intersect = np.where((bboxes[:, 0] < cutoff_x2) &\n",
    "                                   (bboxes[:, 2] > cutoff_x2) &\n",
    "                                   (bboxes[:, 1] < cutoff_y2) &\n",
    "                                   (bboxes[:, 3] > cutoff_y1))\n",
    "        bottom_intersect = np.where((bboxes[:, 0] < cutoff_x2) &\n",
    "                                    (bboxes[:, 2] > cutoff_x1) &\n",
    "                                    (bboxes[:, 1] < cutoff_y1) &\n",
    "                                    (bboxes[:, 3] > cutoff_y1))\n",
    "        left_intersect = np.where((bboxes[:, 0] < cutoff_x1) &\n",
    "                                  (bboxes[:, 2] > cutoff_x1) &\n",
    "                                  (bboxes[:, 1] < cutoff_y2) &\n",
    "                                  (bboxes[:, 3] > cutoff_y1))\n",
    "\n",
    "        # Remove redundant indices. e.g. a bbox which intersects in both right and top.\n",
    "        right_intersect = np.setdiff1d(right_intersect, top_intersect)\n",
    "        right_intersect = np.setdiff1d(right_intersect, bottom_intersect)\n",
    "        right_intersect = np.setdiff1d(right_intersect, left_intersect)\n",
    "        bottom_intersect = np.setdiff1d(bottom_intersect, top_intersect)\n",
    "        bottom_intersect = np.setdiff1d(bottom_intersect, left_intersect)\n",
    "        left_intersect = np.setdiff1d(left_intersect, top_intersect)\n",
    "\n",
    "        bboxes_intersect[:, 1][top_intersect] = cutoff_y2\n",
    "        bboxes_intersect[:, 0][right_intersect] = cutoff_x2\n",
    "        bboxes_intersect[:, 3][bottom_intersect] = cutoff_y1\n",
    "        bboxes_intersect[:, 2][left_intersect] = cutoff_x1\n",
    "\n",
    "        bboxes_intersect[:, 1][top_intersect] = cutoff_y2\n",
    "        bboxes_intersect[:, 0][right_intersect] = cutoff_x2\n",
    "        bboxes_intersect[:, 3][bottom_intersect] = cutoff_y1\n",
    "        bboxes_intersect[:, 2][left_intersect] = cutoff_x1\n",
    "\n",
    "        bboxes_intersect = bboxes_intersect[np.concatenate((top_intersect,\n",
    "                                                            right_intersect,\n",
    "                                                            bottom_intersect,\n",
    "                                                            left_intersect), axis=None)]\n",
    "\n",
    "        # Case 3. Bounding boxes inside cut off patch.\n",
    "        bboxes_to_be_mixed[:, [0, 2]] = bboxes_to_be_mixed[:, [0, 2]].clip(min=cutoff_x1, max=cutoff_x2)\n",
    "        bboxes_to_be_mixed[:, [1, 3]] = bboxes_to_be_mixed[:, [1, 3]].clip(min=cutoff_y1, max=cutoff_y2)\n",
    "\n",
    "        # Integrate all those three cases.\n",
    "        bboxes_cutmix = np.vstack((bboxes_not_intersect, bboxes_intersect, bboxes_to_be_mixed)).astype(int)\n",
    "        bboxes_cutmix = bboxes_cutmix[np.where((bboxes_cutmix[:, 2] - bboxes_cutmix[:, 0]) \\\n",
    "                                               * (bboxes_cutmix[:, 3] - bboxes_cutmix[:, 1]) > 500)]\n",
    "        # End preparing bboxes_cutmix.\n",
    "\n",
    "        return image_cutmix, bboxes_cutmix\n",
    "\n",
    "    def load_mixup_image_and_boxes(self, index):\n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "        r_image, r_boxes = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n",
    "        return (image + r_image) / 2, np.vstack((boxes, r_boxes)).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "#     \"\"\"\n",
    "#     We change the backbone to use the ResNet101 model instead, DETAILS AT:\n",
    "#     https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#modifying-the-model-to-add-a-different-backbone\n",
    "#     \"\"\"\n",
    "#     resnet_101 = torchvision.models.resnet101(pretrained=True)\n",
    "#     layers = list(resnet_101.children())[:-2]\n",
    "#     backbone = torch.nn.Sequential(*layers)\n",
    "#     backbone.out_channels = 2048\n",
    "\n",
    "#     anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "#     # roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "#     #                                             output_size=7,\n",
    "#     #                                             sampling_ratio=2)\n",
    "\n",
    "#     # put the pieces together inside a FasterRCNN model\n",
    "#     model = FasterRCNN(backbone,\n",
    "#                    num_classes=2,\n",
    "#                    rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/58362892/resnet-18-as-backbone-in-faster-r-cnn\n",
    "    \"\"\"\n",
    "    backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n",
    "    model = FasterRCNN(backbone, num_classes=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10% into validation\n",
    "n_validation = int(0.1* len(UNIQ_TRAIN_IMAGE_IDS))\n",
    "valid_ids = UNIQ_TRAIN_IMAGE_IDS[-n_validation:]\n",
    "train_ids = UNIQ_TRAIN_IMAGE_IDS[:-n_validation]\n",
    "\n",
    "df_in_valid = train_df[train_df['image_id'].isin(valid_ids)]\n",
    "df_in_train = train_df[train_df['image_id'].isin(train_ids)]\n",
    "\n",
    "print(\"%i training samples\\n%i validation samples\" % (len(df_in_train[\"image_id\"].unique()), len(df_in_valid[\"image_id\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=512, width=512, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WheatDataset(df_in_train, os.path.join(DATA_DIR, \"train\"), get_train_transforms())\n",
    "valid_dataset = WheatDataset(df_in_valid, os.path.join(DATA_DIR, \"train\"), get_valid_transforms())\n",
    "\n",
    "# since our single getitem returns image, targets. [shape of targets is different depending on the number of bounding boxes in the image] ?\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check the batch is working\n",
    "# batch_of_images, batch_of_targets = next(iter(train_data_loader))\n",
    "# sample_boxes = batch_of_targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
    "# # convert to normal image format\n",
    "# sample_image = batch_of_images[0].permute(1,2,0).cpu().numpy()\n",
    "# plt.imshow(draw_bboxes(sample_boxes, sample_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = Averager()\n",
    "val_losses = Averager()\n",
    "lowest_val_loss = float('inf')\n",
    "\n",
    "# has to be in train mode for both train and valid coz the outputs are different in two cases\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_losses.reset()\n",
    "    val_losses.reset()\n",
    "\n",
    "    for batch_index, (images, targets) in enumerate(train_data_loader):\n",
    "        # move the images and targets to device\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images,targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # track the loss\n",
    "        train_losses.send(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {batch_index} Loss: {loss.item()}\")\n",
    "\n",
    "    # evaluate\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(valid_data_loader):\n",
    "            # move the images and targets to device\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_loss = sum(loss for loss in val_loss_dict.values())\n",
    "\n",
    "            # track the loss\n",
    "            val_losses.send(val_loss.item())\n",
    "\n",
    "    if val_losses.value < lowest_val_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, \"best_model.pth\"))\n",
    "    else:\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    \n",
    "    # print stats\n",
    "    print(f\"Epoch #{epoch} TRAIN LOSS: {train_losses.value} VALIDATION LOSS: {val_losses.value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, 'fasterRCNN_101.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"fasterrcnnresnet101\", \"Faster R-CNN ResNet-101\", new=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}