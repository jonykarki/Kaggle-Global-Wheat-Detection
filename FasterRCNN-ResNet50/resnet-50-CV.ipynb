{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if os.path.abspath(os.pardir) not in sys.path:\n",
    "    sys.path.insert(1, os.path.abspath(os.pardir))\n",
    "import CONFIG\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, re\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "MODELS_OUT_DIR = CONFIG.CFG.DATA.MODELS_OUT\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ON_CPU = DEVICE == torch.device('cpu')\n",
    "TRAIN_BATCH_SIZE = 4 if ON_CPU else 16\n",
    "VALID_BATCH_SIZE = 4 if ON_CPU else 6\n",
    "NUM_EPOCHS = 5 if ON_CPU else 10\n",
    "LEARNING_RATE = 4e-5\n",
    "NEW_COLUMNS = ['x', 'y', 'w', 'h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((147793, 5), (10, 2))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "UNIQ_TRAIN_IMAGE_IDS = train_df[\"image_id\"].unique()\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>222.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>226.0</td>\n      <td>548.0</td>\n      <td>130.0</td>\n      <td>58.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>377.0</td>\n      <td>504.0</td>\n      <td>74.0</td>\n      <td>160.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>95.0</td>\n      <td>109.0</td>\n      <td>107.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>26.0</td>\n      <td>144.0</td>\n      <td>124.0</td>\n      <td>117.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    image_id  width  height   source      x      y      w      h\n0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# separate the bboxes into new columns\n",
    "sep_bboxes = np.stack(train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, new_column in enumerate(NEW_COLUMNS):\n",
    "    train_df[new_column] = sep_bboxes[:, i]\n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(boxes, image, color=(255,0,0)):\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 3\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_train_sample():\n",
    "    image_id = np.random.choice(UNIQ_TRAIN_IMAGE_IDS)\n",
    "    plt.title(image_id)\n",
    "    image = load_image(os.path.join(DATA_DIR, \"train\", f\"{image_id}.jpg\"))\n",
    "    bboxes = (train_df[train_df[\"image_id\"] == image_id][NEW_COLUMNS]).to_numpy()\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    plt.imshow(draw_bboxes(bboxes, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_train_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = df['image_id'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = load_image(os.path.join(self.image_dir, f\"{image_id}.jpg\")).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        boxes = records[NEW_COLUMNS].values\n",
    "        area = boxes[:, 2] * boxes[:, 3]\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # change the co-ordinates into expected [x, y, x+w, y+h] format\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # since all the boxes are wheat, it's all 1s\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        # consider iscrowd false for all the boxes\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = aug['image']\n",
    "            target['boxes'] = torch.as_tensor(aug['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.9),\n",
    "        A.HorizontalFlip(0.5),\n",
    "        A.VerticalFlip(0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ],\n",
    "    p=1.0, \n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=['labels']\n",
    "    ))\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=['labels']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # replace the end predictor with new FastRCNNPredictor with 2 classes (background(0) + wheat(1) (background is always 0))\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features, num_classes=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data_loader, lr_scheduler):\n",
    "    losses = AverageMeter()\n",
    "    for b_idx, (images, targets) in enumerate(train_data_loader):\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.update(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b_idx % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {b_idx} Loss: {loss.item()} LR: {optimizer.param_groups[0]['lr']}\")\n",
    "    if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    print(f\"End of epoch {epoch}. Loss: {losses.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, valid_data_loader):\n",
    "    losses = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(valid_data_loader):\n",
    "            # move the images and targets to device\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_loss = sum(loss for loss in val_loss_dict.values())\n",
    "\n",
    "            losses.update(val_loss.item())\n",
    "    print(f\"Validation Loss: {losses.avg}\\n\")\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n\tnonzero(Tensor input, *, Tensor out)\nConsider using one of the following signatures instead:\n\tnonzero(Tensor input, *, bool as_tuple)\nEpoch: 0 Batch Index: 0 Loss: 4.114666938781738 LR: 4e-05\nEpoch: 0 Batch Index: 50 Loss: 0.7839823961257935 LR: 4e-05\nEpoch: 0 Batch Index: 100 Loss: 0.569884717464447 LR: 4e-05\nEnd of epoch 0. Loss: 0.79402623785303\nValidation Loss: 0.8671898968676304\n\nEpoch: 1 Batch Index: 0 Loss: 0.6435307860374451 LR: 4e-05\nEpoch: 1 Batch Index: 50 Loss: 0.5262898206710815 LR: 4e-05\nEpoch: 1 Batch Index: 100 Loss: 0.5734795331954956 LR: 4e-05\nEnd of epoch 1. Loss: 0.5881745536276635\nValidation Loss: 0.8429211771234553\n\nEpoch: 2 Batch Index: 0 Loss: 0.5695018172264099 LR: 4e-05\nEpoch: 2 Batch Index: 50 Loss: 0.5996235013008118 LR: 4e-05\nEpoch: 2 Batch Index: 100 Loss: 0.4994807243347168 LR: 4e-05\nEnd of epoch 2. Loss: 0.5478065447181675\nValidation Loss: 0.8355283974967105\n\nEpoch: 3 Batch Index: 0 Loss: 0.5175564885139465 LR: 4e-05\nEpoch: 3 Batch Index: 50 Loss: 0.4806326925754547 LR: 4e-05\nEpoch: 3 Batch Index: 100 Loss: 0.47326940298080444 LR: 4e-05\nEnd of epoch 3. Loss: 0.5164873826165571\nValidation Loss: 0.841342771941043\n\nEpoch: 4 Batch Index: 0 Loss: 0.5497444272041321 LR: 4e-05\nEpoch: 4 Batch Index: 50 Loss: 0.46476149559020996 LR: 4e-05\nEpoch: 4 Batch Index: 100 Loss: 0.4615940749645233 LR: 4e-05\nEnd of epoch 4. Loss: 0.49565481907087017\nValidation Loss: 0.8194994444542743\n\nEpoch: 5 Batch Index: 0 Loss: 0.4339888393878937 LR: 4e-05\nEpoch: 5 Batch Index: 50 Loss: 0.4898921251296997 LR: 4e-05\nEpoch: 5 Batch Index: 100 Loss: 0.43279826641082764 LR: 4e-05\nEnd of epoch 5. Loss: 0.4740860684120909\nValidation Loss: 0.8270249119464387\n\nEpoch: 6 Batch Index: 0 Loss: 0.41610369086265564 LR: 4e-05\nEpoch: 6 Batch Index: 50 Loss: 0.4589543342590332 LR: 4e-05\nEpoch: 6 Batch Index: 100 Loss: 0.45101815462112427 LR: 4e-05\nEnd of epoch 6. Loss: 0.46010765517857055\nValidation Loss: 0.833260548558641\n\nEpoch: 7 Batch Index: 0 Loss: 0.4422956109046936 LR: 4e-05\nEpoch: 7 Batch Index: 50 Loss: 0.4055447578430176 LR: 4e-05\nEpoch: 7 Batch Index: 100 Loss: 0.4805167317390442 LR: 4e-05\nEnd of epoch 7. Loss: 0.44067938002288765\nValidation Loss: 0.8635285220881725\n\nEpoch: 8 Batch Index: 0 Loss: 0.4191030263900757 LR: 4e-05\nEpoch: 8 Batch Index: 50 Loss: 0.3892676830291748 LR: 4e-05\nEpoch: 8 Batch Index: 100 Loss: 0.5053895115852356 LR: 4e-05\nEnd of epoch 8. Loss: 0.4263840159203144\nValidation Loss: 0.9208786424170149\n\nEpoch: 9 Batch Index: 0 Loss: 0.45962047576904297 LR: 4e-05\nEpoch: 9 Batch Index: 50 Loss: 0.39020729064941406 LR: 4e-05\nEpoch: 9 Batch Index: 100 Loss: 0.3624080419540405 LR: 4e-05\nEnd of epoch 9. Loss: 0.40978898477892506\nValidation Loss: 0.9469074762247979\n\nEnd of fold 0\n\nEpoch: 0 Batch Index: 0 Loss: 4.478264331817627 LR: 4e-05\nEpoch: 0 Batch Index: 50 Loss: 0.8695690035820007 LR: 4e-05\nEpoch: 0 Batch Index: 100 Loss: 0.6892263293266296 LR: 4e-05\nEnd of epoch 0. Loss: 0.8464845283657101\nValidation Loss: 0.9543519760382936\n\nEpoch: 1 Batch Index: 0 Loss: 0.6006607413291931 LR: 4e-05\nEpoch: 1 Batch Index: 50 Loss: 0.6906017661094666 LR: 4e-05\nEpoch: 1 Batch Index: 100 Loss: 0.6683295965194702 LR: 4e-05\nEnd of epoch 1. Loss: 0.6534146087389465\nValidation Loss: 0.9250203412263951\n\nEpoch: 2 Batch Index: 0 Loss: 0.649759829044342 LR: 4e-05\nEpoch: 2 Batch Index: 50 Loss: 0.5553815960884094 LR: 4e-05\nEpoch: 2 Batch Index: 100 Loss: 0.6100570559501648 LR: 4e-05\nEnd of epoch 2. Loss: 0.6135374168132214\nValidation Loss: 0.88128163078998\n\nEpoch: 3 Batch Index: 0 Loss: 0.5727935433387756 LR: 4e-05\nEpoch: 3 Batch Index: 50 Loss: 0.5813136100769043 LR: 4e-05\nEpoch: 3 Batch Index: 100 Loss: 0.6397804617881775 LR: 4e-05\nEnd of epoch 3. Loss: 0.5887803612871373\nValidation Loss: 0.8986068206264618\n\nEpoch: 4 Batch Index: 0 Loss: 0.5485454797744751 LR: 4e-05\nEpoch: 4 Batch Index: 50 Loss: 0.6298757791519165 LR: 4e-05\nEpoch: 4 Batch Index: 100 Loss: 0.558039128780365 LR: 4e-05\nEnd of epoch 4. Loss: 0.5649331108475408\nValidation Loss: 0.9109526430355742\n\nEpoch: 5 Batch Index: 0 Loss: 0.5967203378677368 LR: 4e-05\nEpoch: 5 Batch Index: 50 Loss: 0.561486005783081 LR: 4e-05\nEpoch: 5 Batch Index: 100 Loss: 0.543152928352356 LR: 4e-05\nEnd of epoch 5. Loss: 0.5487405683131928\nValidation Loss: 0.9526056044913352\n\nEpoch: 6 Batch Index: 0 Loss: 0.520706057548523 LR: 4e-05\nEpoch: 6 Batch Index: 50 Loss: 0.4683724641799927 LR: 4e-05\nEpoch: 6 Batch Index: 100 Loss: 0.5583100318908691 LR: 4e-05\nEnd of epoch 6. Loss: 0.5281299451986948\nValidation Loss: 0.9420732296210654\n\nEpoch: 7 Batch Index: 0 Loss: 0.515239953994751 LR: 4e-05\nEpoch: 7 Batch Index: 50 Loss: 0.4950368404388428 LR: 4e-05\nEpoch: 7 Batch Index: 100 Loss: 0.5207124948501587 LR: 4e-05\nEnd of epoch 7. Loss: 0.5134964714236293\nValidation Loss: 0.8955131995868175\n\nEpoch: 8 Batch Index: 0 Loss: 0.49041008949279785 LR: 4e-05\nEpoch: 8 Batch Index: 50 Loss: 0.5323339700698853 LR: 4e-05\nEpoch: 8 Batch Index: 100 Loss: 0.5120837688446045 LR: 4e-05\nEnd of epoch 8. Loss: 0.4929477107440326\nValidation Loss: 0.9460616666585842\n\nEpoch: 9 Batch Index: 0 Loss: 0.44398829340934753 LR: 4e-05\nEpoch: 9 Batch Index: 50 Loss: 0.40440598130226135 LR: 4e-05\nEpoch: 9 Batch Index: 100 Loss: 0.49694687128067017 LR: 4e-05\nEnd of epoch 9. Loss: 0.47834985623968407\nValidation Loss: 1.0615746592904658\n\nEnd of fold 1\n\nEpoch: 0 Batch Index: 0 Loss: 4.066361904144287 LR: 4e-05\nEpoch: 0 Batch Index: 50 Loss: 0.7495956420898438 LR: 4e-05\nEpoch: 0 Batch Index: 100 Loss: 0.7233127355575562 LR: 4e-05\nEnd of epoch 0. Loss: 0.8669622358700908\nValidation Loss: 0.7895872183936707\n\nEpoch: 1 Batch Index: 0 Loss: 0.6978759765625 LR: 4e-05\nEpoch: 1 Batch Index: 50 Loss: 0.6646705865859985 LR: 4e-05\nEpoch: 1 Batch Index: 100 Loss: 0.6269592642784119 LR: 4e-05\nEnd of epoch 1. Loss: 0.6634182887719878\nValidation Loss: 0.7513856960737959\n\nEpoch: 2 Batch Index: 0 Loss: 0.7276953458786011 LR: 4e-05\nEpoch: 2 Batch Index: 50 Loss: 0.6408402919769287 LR: 4e-05\nEpoch: 2 Batch Index: 100 Loss: 0.6087952852249146 LR: 4e-05\nEnd of epoch 2. Loss: 0.6212938144697365\nValidation Loss: 0.7067774676896156\n\nEpoch: 3 Batch Index: 0 Loss: 0.5700340270996094 LR: 4e-05\nEpoch: 3 Batch Index: 50 Loss: 0.6661314964294434 LR: 4e-05\nEpoch: 3 Batch Index: 100 Loss: 0.555262565612793 LR: 4e-05\nEnd of epoch 3. Loss: 0.592085747431356\nValidation Loss: 0.7573950152764929\n\nEpoch: 4 Batch Index: 0 Loss: 0.5075620412826538 LR: 4e-05\nEpoch: 4 Batch Index: 50 Loss: 0.6104949712753296 LR: 4e-05\nEpoch: 4 Batch Index: 100 Loss: 0.5818433165550232 LR: 4e-05\nEnd of epoch 4. Loss: 0.5715138210472486\nValidation Loss: 0.7216790334341374\n\nEpoch: 5 Batch Index: 0 Loss: 0.5585451126098633 LR: 4e-05\nEpoch: 5 Batch Index: 50 Loss: 0.5172154307365417 LR: 4e-05\nEpoch: 5 Batch Index: 100 Loss: 0.5129069089889526 LR: 4e-05\nEnd of epoch 5. Loss: 0.5519665437387237\nValidation Loss: 0.7532560029562484\n\nEpoch: 6 Batch Index: 0 Loss: 0.5432788133621216 LR: 4e-05\nEpoch: 6 Batch Index: 50 Loss: 0.5002840161323547 LR: 4e-05\nEpoch: 6 Batch Index: 100 Loss: 0.550250768661499 LR: 4e-05\nEnd of epoch 6. Loss: 0.5327982765140263\nValidation Loss: 0.7351616193005379\n\nEpoch: 7 Batch Index: 0 Loss: 0.5019338726997375 LR: 4e-05\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8c2ce2ce9083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cc3f1a178a6e>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_data_loader, lr_scheduler)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_gt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             loss_objectness, loss_rpn_box_reg = self.compute_loss(\n\u001b[0;32m--> 496\u001b[0;31m                 objectness, pred_bbox_deltas, labels, regression_targets)\n\u001b[0m\u001b[1;32m    497\u001b[0m             losses = {\n\u001b[1;32m    498\u001b[0m                 \u001b[0;34m\"loss_objectness\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_objectness\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, objectness, pred_bbox_deltas, labels, regression_targets)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \"\"\"\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0msampled_pos_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_neg_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfg_bg_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0msampled_pos_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_pos_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0msampled_neg_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_neg_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, matched_idxs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mneg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatched_idxs_per_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatched_idxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_idxs_per_image\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_idxs_per_image\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train 5 folds\n",
    "for idx, (train_ids, valid_ids) in enumerate(kf.split(UNIQ_TRAIN_IMAGE_IDS)):\n",
    "    ids_train = [UNIQ_TRAIN_IMAGE_IDS[k] for k in train_ids]\n",
    "    ids_valid = [UNIQ_TRAIN_IMAGE_IDS[k] for k in valid_ids]\n",
    "\n",
    "    df_train = train_df[train_df['image_id'].isin(ids_train)]\n",
    "    df_valid = train_df[train_df['image_id'].isin(ids_valid)]\n",
    "\n",
    "    train_dataset = WheatDataset(df_train, os.path.join(DATA_DIR, \"train\"), get_train_transforms())\n",
    "    valid_dataset = WheatDataset(df_valid, os.path.join(DATA_DIR, \"train\"), get_valid_transforms())\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    model = get_model()\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    lr_scheduler = None\n",
    "    # params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    model.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_epoch(model, optimizer, train_data_loader, lr_scheduler)\n",
    "        val_loss = validate_epoch(model, valid_data_loader)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, f\"best_model_{idx}.pth\"))\n",
    "    print(f\"End of fold {idx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"gwdresnet50cv\", \"GWD ResNet-50 CV\", new=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}