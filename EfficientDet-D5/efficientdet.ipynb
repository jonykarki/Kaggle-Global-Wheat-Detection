{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODELS_DIR = os.path.join(\"/content\", \"pretrained\")\n",
    "EFFICIENTDET_PYTORCH_DIR = os.path.join(\"/content\", \"efficientdet-pytorch\")\n",
    "EFFICIENTDET_PYTORCH_GIT_LINK = \"https://github.com/rwightman/efficientdet-pytorch.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, subprocess\n",
    "if not all(x in sys.path for x in [os.path.abspath(os.pardir), EFFICIENTDET_PYTORCH_DIR]):\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "    sys.path.insert(0, EFFICIENTDET_PYTORCH_DIR)\n",
    "import CONFIG\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_efficientdet():\n",
    "    if not all([os.path.exists(PRETRAINED_MODELS_DIR), os.path.exists(EFFICIENTDET_PYTORCH_DIR)]):\n",
    "        os.makedirs(PRETRAINED_MODELS_DIR, exist_ok=True)\n",
    "        subprocess.call(f\"git clone {EFFICIENTDET_PYTORCH_GIT_LINK}\", shell=True, cwd=CONFIG.CFG.BASEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_efficientdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from glob import glob\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "MODELS_OUT_DIR = CONFIG.CFG.DATA.MODELS_OUT\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ON_CPU = DEVICE == torch.device('cpu')\n",
    "TRAIN_BATCH_SIZE = 4 if ON_CPU else 5\n",
    "VALID_BATCH_SIZE = 4 if ON_CPU else 6\n",
    "NUM_EPOCHS = 5 if ON_CPU else 10\n",
    "LEARNING_RATE = 4e-5\n",
    "NEW_COLUMNS = ['x', 'y', 'w', 'h']\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((147793, 5), (10, 2))"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "UNIQ_TRAIN_IMAGE_IDS = train_df[\"image_id\"].unique()\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>222.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>226.0</td>\n      <td>548.0</td>\n      <td>130.0</td>\n      <td>58.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>377.0</td>\n      <td>504.0</td>\n      <td>74.0</td>\n      <td>160.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>95.0</td>\n      <td>109.0</td>\n      <td>107.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>26.0</td>\n      <td>144.0</td>\n      <td>124.0</td>\n      <td>117.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    image_id  width  height   source      x      y      w      h\n0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# separate the bboxes into new columns\n",
    "sep_bboxes = np.stack(train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, new_column in enumerate(NEW_COLUMNS):\n",
    "    train_df[new_column] = sep_bboxes[:, i]\n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.loc[:, \"fold\"] = -1\n",
    "kf = KFold(shuffle=True, random_state=42)\n",
    "for fold, (train_ids, valid_ids) in enumerate(kf.split(UNIQ_TRAIN_IMAGE_IDS)):\n",
    "    ids_valid = [UNIQ_TRAIN_IMAGE_IDS[k] for k in valid_ids]\n",
    "\n",
    "    indices = train_df[train_df['image_id'].isin(ids_valid)].index\n",
    "    train_df.loc[indices, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55997</th>\n      <td>1cf002747</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>436.0</td>\n      <td>524.0</td>\n      <td>115.0</td>\n      <td>53.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>22445</th>\n      <td>b61e3461d</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>475.0</td>\n      <td>402.0</td>\n      <td>81.0</td>\n      <td>97.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51867</th>\n      <td>e2018867c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>inrae_1</td>\n      <td>422.0</td>\n      <td>180.0</td>\n      <td>106.0</td>\n      <td>163.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>93085</th>\n      <td>302fd5e69</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>368.0</td>\n      <td>279.0</td>\n      <td>62.0</td>\n      <td>44.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>103557</th>\n      <td>14da8934d</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>780.0</td>\n      <td>595.0</td>\n      <td>54.0</td>\n      <td>54.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96795</th>\n      <td>27f0a8188</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>507.0</td>\n      <td>909.0</td>\n      <td>62.0</td>\n      <td>24.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>67884</th>\n      <td>e891a6cff</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>0.0</td>\n      <td>449.0</td>\n      <td>129.0</td>\n      <td>80.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10723</th>\n      <td>b95fd89e3</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>572.0</td>\n      <td>291.0</td>\n      <td>77.0</td>\n      <td>78.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35637</th>\n      <td>e4b256788</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>240.0</td>\n      <td>114.0</td>\n      <td>96.0</td>\n      <td>100.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>17247</th>\n      <td>b0a32bf24</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>967.0</td>\n      <td>524.0</td>\n      <td>56.0</td>\n      <td>47.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         image_id  width  height     source      x      y      w      h  fold\n55997   1cf002747   1024    1024     ethz_1  436.0  524.0  115.0   53.0     2\n22445   b61e3461d   1024    1024  arvalis_1  475.0  402.0   81.0   97.0     2\n51867   e2018867c   1024    1024    inrae_1  422.0  180.0  106.0  163.0     2\n93085   302fd5e69   1024    1024     ethz_1  368.0  279.0   62.0   44.0     3\n103557  14da8934d   1024    1024     ethz_1  780.0  595.0   54.0   54.0     2\n96795   27f0a8188   1024    1024     ethz_1  507.0  909.0   62.0   24.0     4\n67884   e891a6cff   1024    1024     ethz_1    0.0  449.0  129.0   80.0     3\n10723   b95fd89e3   1024    1024  arvalis_1  572.0  291.0   77.0   78.0     0\n35637   e4b256788   1024    1024  arvalis_1  240.0  114.0   96.0  100.0     4\n17247   b0a32bf24   1024    1024  arvalis_1  967.0  524.0   56.0   47.0     2"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = df['image_id'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = load_image(os.path.join(self.image_dir, f\"{image_id}.jpg\")).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        boxes = records[NEW_COLUMNS].values\n",
    "        area = boxes[:, 2] * boxes[:, 3]\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # change the co-ordinates into expected [x, y, x+w, y+h] format\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # since all the boxes are wheat, it's all 1s\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            while len(aug['bboxes']) == 0:\n",
    "                aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = aug['image']\n",
    "            target['boxes'] = torch.as_tensor(aug['bboxes'], dtype=torch.float32)\n",
    "\n",
    "            # format yxyx\n",
    "            target['boxes'][:, [0,1,2,3]] = target['boxes'][:, [1,0,3,2]]\n",
    "            target['labels'] = torch.stack(aug['labels'])\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "train_dataset = WheatDataset(\n",
    "    train_df[train_df['fold'] != fold],\n",
    "    os.path.join(DATA_DIR, \"train\"),\n",
    "    transforms=get_train_transforms(),\n",
    ")\n",
    "\n",
    "validation_dataset = WheatDataset(\n",
    "    train_df[train_df['fold'] == fold],\n",
    "    os.path.join(DATA_DIR, \"train\"),\n",
    "    transforms=get_valid_transforms(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        num_workers=1,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    sampler=SequentialSampler(validation_dataset),\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "def get_model():\n",
    "    model_config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    model = EfficientDet(model_config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load(os.path.join(PRETRAINED_MODELS_DIR, 'tf_efficientdet_d5-ef44aea8.pth'))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model_config.num_classes = 1\n",
    "    model_config.image_size = IMAGE_SIZE\n",
    "    model.class_net = HeadNet(model_config, num_outputs=model_config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "    return DetBenchTrain(model, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = get_model()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data_loader, lr_scheduler):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    for b_idx, (images, targets) in enumerate(train_data_loader):\n",
    "        images = torch.stack(images).to(DEVICE).float()\n",
    "\n",
    "        target = {\n",
    "            'bbox': [],\n",
    "            'cls': []\n",
    "        }\n",
    "\n",
    "        for d in targets:\n",
    "            target['bbox'].append(d['boxes'].to(DEVICE).float())\n",
    "            target['cls'].append(d['labels'].to(DEVICE).float())\n",
    "\n",
    "        output = model(images, target)\n",
    "        loss = output['loss']\n",
    "\n",
    "        losses.update(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b_idx % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {b_idx} Loss: {loss.item()} LR: {optimizer.param_groups[0]['lr']}\")\n",
    "    if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    print(f\"End of epoch {epoch}. Loss: {losses.avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, valid_data_loader):\n",
    "    losses = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(valid_data_loader):\n",
    "            # move the images and targets to device\n",
    "            images = torch.stack(images).to(DEVICE).float()\n",
    "            target = {\n",
    "                'bbox': [],\n",
    "                'cls': []\n",
    "            }\n",
    "\n",
    "            for d in targets:\n",
    "                target['bbox'].append(d['boxes'].to(DEVICE).float())\n",
    "                target['cls'].append(d['labels'].to(DEVICE).float())\n",
    "\n",
    "            output = model(images, target)\n",
    "            val_loss = output['loss']\n",
    "\n",
    "            losses.update(val_loss.item())\n",
    "    print(f\"Validation Loss: {losses.avg}\\n\")\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    best_val = float('inf')\n",
    "    train_epoch(model, optimizer, train_data_loader, lr_scheduler)\n",
    "    val_loss = validate_epoch(model, valid_data_loader)  \n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        print(\"SAVING MODEL\")\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, f\"best_model_{idx}.pth\"))\n",
    "    print(f\"End of epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"efficientdetd5\", \"Efficient Det D5\", new=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}