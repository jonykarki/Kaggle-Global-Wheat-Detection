{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODELS_DIR = os.path.join(\"/content\", \"pretrained\")\n",
    "EFFICIENTDET_PYTORCH_DIR = os.path.join(\"/content\", \"efficientdet-pytorch\")\n",
    "EFFICIENTDET_PYTORCH_GIT_LINK = \"https://github.com/rwightman/efficientdet-pytorch.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, subprocess\n",
    "if not all(x in sys.path for x in [os.path.abspath(os.pardir), EFFICIENTDET_PYTORCH_DIR]):\n",
    "    sys.path.insert(0, os.path.abspath(os.pardir))\n",
    "    sys.path.insert(0, EFFICIENTDET_PYTORCH_DIR)\n",
    "import CONFIG\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_efficientdet():\n",
    "    if not all([os.path.exists(PRETRAINED_MODELS_DIR), os.path.exists(EFFICIENTDET_PYTORCH_DIR)]):\n",
    "        os.makedirs(PRETRAINED_MODELS_DIR, exist_ok=True)\n",
    "        subprocess.call(f\"git clone {EFFICIENTDET_PYTORCH_GIT_LINK}\", shell=True, cwd=CONFIG.CFG.BASEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_efficientdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from glob import glob\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = CONFIG.CFG.DATA.BASE\n",
    "MODELS_OUT_DIR = CONFIG.CFG.DATA.MODELS_OUT\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "ON_CPU = DEVICE == torch.device('cpu')\n",
    "TRAIN_BATCH_SIZE = 4 if ON_CPU else 5\n",
    "VALID_BATCH_SIZE = 4 if ON_CPU else 6\n",
    "NUM_EPOCHS = 5 if ON_CPU else 10\n",
    "LEARNING_RATE = 4e-5\n",
    "NEW_COLUMNS = ['x', 'y', 'w', 'h']\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((147793, 5), (10, 2))"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "UNIQ_TRAIN_IMAGE_IDS = train_df[\"image_id\"].unique()\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>222.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>226.0</td>\n      <td>548.0</td>\n      <td>130.0</td>\n      <td>58.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>377.0</td>\n      <td>504.0</td>\n      <td>74.0</td>\n      <td>160.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>834.0</td>\n      <td>95.0</td>\n      <td>109.0</td>\n      <td>107.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>usask_1</td>\n      <td>26.0</td>\n      <td>144.0</td>\n      <td>124.0</td>\n      <td>117.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    image_id  width  height   source      x      y      w      h\n0  b6ab77fd7   1024    1024  usask_1  834.0  222.0   56.0   36.0\n1  b6ab77fd7   1024    1024  usask_1  226.0  548.0  130.0   58.0\n2  b6ab77fd7   1024    1024  usask_1  377.0  504.0   74.0  160.0\n3  b6ab77fd7   1024    1024  usask_1  834.0   95.0  109.0  107.0\n4  b6ab77fd7   1024    1024  usask_1   26.0  144.0  124.0  117.0"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# separate the bboxes into new columns\n",
    "sep_bboxes = np.stack(train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, new_column in enumerate(NEW_COLUMNS):\n",
    "    train_df[new_column] = sep_bboxes[:, i]\n",
    "train_df.drop(columns=['bbox'], inplace=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.loc[:, \"fold\"] = -1\n",
    "kf = KFold(shuffle=True, random_state=42)\n",
    "for fold, (train_ids, valid_ids) in enumerate(kf.split(UNIQ_TRAIN_IMAGE_IDS)):\n",
    "    ids_valid = [UNIQ_TRAIN_IMAGE_IDS[k] for k in valid_ids]\n",
    "\n",
    "    indices = train_df[train_df['image_id'].isin(ids_valid)].index\n",
    "    train_df.loc[indices, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>source</th>\n      <th>x</th>\n      <th>y</th>\n      <th>w</th>\n      <th>h</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55997</th>\n      <td>1cf002747</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>436.0</td>\n      <td>524.0</td>\n      <td>115.0</td>\n      <td>53.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>22445</th>\n      <td>b61e3461d</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>475.0</td>\n      <td>402.0</td>\n      <td>81.0</td>\n      <td>97.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>51867</th>\n      <td>e2018867c</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>inrae_1</td>\n      <td>422.0</td>\n      <td>180.0</td>\n      <td>106.0</td>\n      <td>163.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>93085</th>\n      <td>302fd5e69</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>368.0</td>\n      <td>279.0</td>\n      <td>62.0</td>\n      <td>44.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>103557</th>\n      <td>14da8934d</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>780.0</td>\n      <td>595.0</td>\n      <td>54.0</td>\n      <td>54.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96795</th>\n      <td>27f0a8188</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>507.0</td>\n      <td>909.0</td>\n      <td>62.0</td>\n      <td>24.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>67884</th>\n      <td>e891a6cff</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>ethz_1</td>\n      <td>0.0</td>\n      <td>449.0</td>\n      <td>129.0</td>\n      <td>80.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10723</th>\n      <td>b95fd89e3</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>572.0</td>\n      <td>291.0</td>\n      <td>77.0</td>\n      <td>78.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35637</th>\n      <td>e4b256788</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>240.0</td>\n      <td>114.0</td>\n      <td>96.0</td>\n      <td>100.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>17247</th>\n      <td>b0a32bf24</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>arvalis_1</td>\n      <td>967.0</td>\n      <td>524.0</td>\n      <td>56.0</td>\n      <td>47.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "         image_id  width  height     source      x      y      w      h  fold\n55997   1cf002747   1024    1024     ethz_1  436.0  524.0  115.0   53.0     2\n22445   b61e3461d   1024    1024  arvalis_1  475.0  402.0   81.0   97.0     2\n51867   e2018867c   1024    1024    inrae_1  422.0  180.0  106.0  163.0     2\n93085   302fd5e69   1024    1024     ethz_1  368.0  279.0   62.0   44.0     3\n103557  14da8934d   1024    1024     ethz_1  780.0  595.0   54.0   54.0     2\n96795   27f0a8188   1024    1024     ethz_1  507.0  909.0   62.0   24.0     4\n67884   e891a6cff   1024    1024     ethz_1    0.0  449.0  129.0   80.0     3\n10723   b95fd89e3   1024    1024  arvalis_1  572.0  291.0   77.0   78.0     0\n35637   e4b256788   1024    1024  arvalis_1  240.0  114.0   96.0  100.0     4\n17247   b0a32bf24   1024    1024  arvalis_1  967.0  524.0   56.0   47.0     2"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    assert image is not None, f\"IMAGE NOT FOUND AT {image_path}\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WheatDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = df['image_id'].unique()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = load_image(os.path.join(self.image_dir, f\"{image_id}.jpg\")).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "        boxes = records[NEW_COLUMNS].values\n",
    "        area = boxes[:, 2] * boxes[:, 3]\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "        # change the co-ordinates into expected [x, y, x+w, y+h] format\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # since all the boxes are wheat, it's all 1s\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            while len(aug['bboxes']) == 0:\n",
    "                aug = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = aug['image']\n",
    "            target['boxes'] = torch.as_tensor(aug['bboxes'], dtype=torch.float32)\n",
    "\n",
    "            # format yxyx\n",
    "            target['boxes'][:, [0,1,2,3]] = target['boxes'][:, [1,0,3,2]]\n",
    "            target['labels'] = torch.stack(aug['labels'])\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "train_dataset = WheatDataset(\n",
    "    train_df[train_df['fold'] != fold],\n",
    "    os.path.join(DATA_DIR, \"train\"),\n",
    "    transforms=get_train_transforms(),\n",
    ")\n",
    "\n",
    "validation_dataset = WheatDataset(\n",
    "    train_df[train_df['fold'] == fold],\n",
    "    os.path.join(DATA_DIR, \"train\"),\n",
    "    transforms=get_valid_transforms(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        num_workers=1,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    sampler=SequentialSampler(validation_dataset),\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "def get_model():\n",
    "    model_config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    model = EfficientDet(model_config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load(os.path.join(PRETRAINED_MODELS_DIR, 'tf_efficientdet_d5-ef44aea8.pth'))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model_config.num_classes = 1\n",
    "    model_config.image_size = IMAGE_SIZE\n",
    "    model.class_net = HeadNet(model_config, num_outputs=model_config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "    return DetBenchTrain(model, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = get_model()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data_loader, lr_scheduler):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    for b_idx, (images, targets) in enumerate(train_data_loader):\n",
    "        images = torch.stack(images).to(DEVICE).float()\n",
    "\n",
    "        target = {\n",
    "            'bbox': [],\n",
    "            'cls': []\n",
    "        }\n",
    "\n",
    "        for d in targets:\n",
    "            target['bbox'].append(d['boxes'].to(DEVICE).float())\n",
    "            target['cls'].append(d['labels'].to(DEVICE).float())\n",
    "\n",
    "        output = model(images, target)\n",
    "        loss = output['loss']\n",
    "\n",
    "        losses.update(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b_idx % 50 == 0:\n",
    "            print(f\"Epoch: {epoch} Batch Index: {b_idx} Loss: {loss.item()} LR: {optimizer.param_groups[0]['lr']}\")\n",
    "    if lr_scheduler is not None:\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, valid_data_loader):\n",
    "    losses = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(valid_data_loader):\n",
    "            # move the images and targets to device\n",
    "            images = torch.stack(images).to(DEVICE).float()\n",
    "            target = {\n",
    "                'bbox': [],\n",
    "                'cls': []\n",
    "            }\n",
    "\n",
    "            for d in targets:\n",
    "                target['bbox'].append(d['boxes'].to(DEVICE).float())\n",
    "                target['cls'].append(d['labels'].to(DEVICE).float())\n",
    "\n",
    "            output = model(images, target)\n",
    "            val_loss = output['loss']\n",
    "\n",
    "            losses.update(val_loss.item())\n",
    "    print(f\"Validation Loss: {losses.avg}\\n\")\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch: 0 Batch Index: 0 Loss: 16.026317596435547 LR: 4e-05\nEpoch: 0 Batch Index: 50 Loss: 9.414063453674316 LR: 4e-05\nEpoch: 0 Batch Index: 100 Loss: 9.317319869995117 LR: 4e-05\nEpoch: 0 Batch Index: 150 Loss: 6.409059047698975 LR: 4e-05\nEpoch: 0 Batch Index: 200 Loss: 13.012895584106445 LR: 4e-05\nEpoch: 0 Batch Index: 250 Loss: 5.073112964630127 LR: 4e-05\nEpoch: 0 Batch Index: 300 Loss: 4.860930919647217 LR: 4e-05\nEpoch: 0 Batch Index: 350 Loss: 4.368788719177246 LR: 4e-05\nEpoch: 0 Batch Index: 400 Loss: 4.7976274490356445 LR: 4e-05\nEpoch: 0 Batch Index: 450 Loss: 2.60953688621521 LR: 4e-05\nEpoch: 0 Batch Index: 500 Loss: 3.774632215499878 LR: 4e-05\nValidation Loss: 2.838133576696953\n\nSAVING MODEL\nEnd of epoch 0\nEpoch: 1 Batch Index: 0 Loss: 3.144812822341919 LR: 4e-05\nEpoch: 1 Batch Index: 50 Loss: 3.4179797172546387 LR: 4e-05\nEpoch: 1 Batch Index: 100 Loss: 1.7903990745544434 LR: 4e-05\nEpoch: 1 Batch Index: 150 Loss: 2.017256736755371 LR: 4e-05\nEpoch: 1 Batch Index: 200 Loss: 1.5519170761108398 LR: 4e-05\nEpoch: 1 Batch Index: 250 Loss: 1.6934139728546143 LR: 4e-05\nEpoch: 1 Batch Index: 300 Loss: 1.4811947345733643 LR: 4e-05\nEpoch: 1 Batch Index: 350 Loss: 1.7209577560424805 LR: 4e-05\nEpoch: 1 Batch Index: 400 Loss: 1.2063926458358765 LR: 4e-05\nEpoch: 1 Batch Index: 450 Loss: 1.4646985530853271 LR: 4e-05\nEpoch: 1 Batch Index: 500 Loss: 0.9716745018959045 LR: 4e-05\nValidation Loss: 0.9962518684631956\n\nSAVING MODEL\nEnd of epoch 1\nEpoch: 2 Batch Index: 0 Loss: 1.0248727798461914 LR: 4e-05\nEpoch: 2 Batch Index: 50 Loss: 0.9024203419685364 LR: 4e-05\nEpoch: 2 Batch Index: 100 Loss: 0.8833281993865967 LR: 4e-05\nEpoch: 2 Batch Index: 150 Loss: 0.8177884817123413 LR: 4e-05\nEpoch: 2 Batch Index: 200 Loss: 0.7657334804534912 LR: 4e-05\nEpoch: 2 Batch Index: 250 Loss: 0.8493303060531616 LR: 4e-05\nEpoch: 2 Batch Index: 300 Loss: 0.6971211433410645 LR: 4e-05\nEpoch: 2 Batch Index: 350 Loss: 0.7602720856666565 LR: 4e-05\nEpoch: 2 Batch Index: 400 Loss: 0.6866443753242493 LR: 4e-05\nEpoch: 2 Batch Index: 450 Loss: 0.6186679601669312 LR: 4e-05\nEpoch: 2 Batch Index: 500 Loss: 0.653263509273529 LR: 4e-05\nValidation Loss: 0.6503236776432105\n\nSAVING MODEL\nEnd of epoch 2\nEpoch: 3 Batch Index: 0 Loss: 0.6096097826957703 LR: 4e-05\nEpoch: 3 Batch Index: 50 Loss: 0.684308648109436 LR: 4e-05\nEpoch: 3 Batch Index: 100 Loss: 0.6075506806373596 LR: 4e-05\nEpoch: 3 Batch Index: 150 Loss: 0.5880634784698486 LR: 4e-05\nEpoch: 3 Batch Index: 200 Loss: 0.47263920307159424 LR: 4e-05\nEpoch: 3 Batch Index: 250 Loss: 0.7173945903778076 LR: 4e-05\nEpoch: 3 Batch Index: 300 Loss: 0.5016722083091736 LR: 4e-05\nEpoch: 3 Batch Index: 350 Loss: 0.5905982255935669 LR: 4e-05\nEpoch: 3 Batch Index: 400 Loss: 0.6481356024742126 LR: 4e-05\nEpoch: 3 Batch Index: 450 Loss: 0.5961378812789917 LR: 4e-05\nEpoch: 3 Batch Index: 500 Loss: 0.6783896684646606 LR: 4e-05\nValidation Loss: 0.5545216574605587\n\nSAVING MODEL\nEnd of epoch 3\nEpoch: 4 Batch Index: 0 Loss: 0.49750447273254395 LR: 4e-05\nEpoch: 4 Batch Index: 50 Loss: 0.4786304533481598 LR: 4e-05\nEpoch: 4 Batch Index: 100 Loss: 0.5319671034812927 LR: 4e-05\nEpoch: 4 Batch Index: 150 Loss: 0.4896286129951477 LR: 4e-05\nEpoch: 4 Batch Index: 200 Loss: 0.4795246720314026 LR: 4e-05\nEpoch: 4 Batch Index: 250 Loss: 0.5200241804122925 LR: 4e-05\nEpoch: 4 Batch Index: 300 Loss: 0.43308305740356445 LR: 4e-05\nEpoch: 4 Batch Index: 350 Loss: 0.5124360918998718 LR: 4e-05\nEpoch: 4 Batch Index: 400 Loss: 0.5750732421875 LR: 4e-05\nEpoch: 4 Batch Index: 450 Loss: 0.4914989471435547 LR: 4e-05\nEpoch: 4 Batch Index: 500 Loss: 0.5340561866760254 LR: 4e-05\nValidation Loss: 0.5118326444541459\n\nSAVING MODEL\nEnd of epoch 4\nEpoch: 5 Batch Index: 0 Loss: 0.5294259190559387 LR: 4e-05\nEpoch: 5 Batch Index: 50 Loss: 0.46467679738998413 LR: 4e-05\nEpoch: 5 Batch Index: 100 Loss: 0.4321201741695404 LR: 4e-05\nEpoch: 5 Batch Index: 150 Loss: 0.4931544065475464 LR: 4e-05\nEpoch: 5 Batch Index: 200 Loss: 0.3988698720932007 LR: 4e-05\nEpoch: 5 Batch Index: 250 Loss: 0.4479098320007324 LR: 4e-05\nEpoch: 5 Batch Index: 300 Loss: 0.4622865319252014 LR: 4e-05\nEpoch: 5 Batch Index: 350 Loss: 0.4651244878768921 LR: 4e-05\nEpoch: 5 Batch Index: 400 Loss: 0.4480785131454468 LR: 4e-05\nEpoch: 5 Batch Index: 450 Loss: 0.4139191210269928 LR: 4e-05\nEpoch: 5 Batch Index: 500 Loss: 0.4077486991882324 LR: 4e-05\nValidation Loss: 0.48854963573734317\n\nSAVING MODEL\nEnd of epoch 5\nEpoch: 6 Batch Index: 0 Loss: 0.4415731430053711 LR: 4e-05\nEpoch: 6 Batch Index: 50 Loss: 0.3710402846336365 LR: 4e-05\nEpoch: 6 Batch Index: 100 Loss: 0.5075582265853882 LR: 4e-05\nEpoch: 6 Batch Index: 150 Loss: 0.46629294753074646 LR: 4e-05\nEpoch: 6 Batch Index: 200 Loss: 0.4255116879940033 LR: 4e-05\nEpoch: 6 Batch Index: 250 Loss: 0.45555946230888367 LR: 4e-05\nEpoch: 6 Batch Index: 300 Loss: 0.44182056188583374 LR: 4e-05\nEpoch: 6 Batch Index: 350 Loss: 0.4281802177429199 LR: 4e-05\nEpoch: 6 Batch Index: 400 Loss: 0.4325401186943054 LR: 4e-05\nEpoch: 6 Batch Index: 450 Loss: 0.4918503761291504 LR: 4e-05\nEpoch: 6 Batch Index: 500 Loss: 0.4496038556098938 LR: 4e-05\nValidation Loss: 0.4705017488614648\n\nSAVING MODEL\nEnd of epoch 6\nEpoch: 7 Batch Index: 0 Loss: 0.3982456624507904 LR: 4e-05\nEpoch: 7 Batch Index: 50 Loss: 0.4281626045703888 LR: 4e-05\nEpoch: 7 Batch Index: 100 Loss: 0.43852585554122925 LR: 4e-05\nEpoch: 7 Batch Index: 150 Loss: 0.4059518277645111 LR: 4e-05\nEpoch: 7 Batch Index: 200 Loss: 0.4874489903450012 LR: 4e-05\nEpoch: 7 Batch Index: 250 Loss: 0.367899626493454 LR: 4e-05\nEpoch: 7 Batch Index: 300 Loss: 0.5420559644699097 LR: 4e-05\nEpoch: 7 Batch Index: 350 Loss: 0.506920576095581 LR: 4e-05\nEpoch: 7 Batch Index: 400 Loss: 0.4379788935184479 LR: 4e-05\nEpoch: 7 Batch Index: 450 Loss: 0.5298258066177368 LR: 4e-05\nEpoch: 7 Batch Index: 500 Loss: 0.48131072521209717 LR: 4e-05\nValidation Loss: 0.4715209495177311\n\nSAVING MODEL\nEnd of epoch 7\nEpoch: 8 Batch Index: 0 Loss: 0.41745883226394653 LR: 4e-05\nEpoch: 8 Batch Index: 50 Loss: 0.43092888593673706 LR: 4e-05\nEpoch: 8 Batch Index: 100 Loss: 0.5442314743995667 LR: 4e-05\nEpoch: 8 Batch Index: 150 Loss: 0.44454264640808105 LR: 4e-05\nEpoch: 8 Batch Index: 200 Loss: 0.5082594752311707 LR: 4e-05\nEpoch: 8 Batch Index: 250 Loss: 0.4167533814907074 LR: 4e-05\nEpoch: 8 Batch Index: 300 Loss: 0.42147690057754517 LR: 4e-05\nEpoch: 8 Batch Index: 350 Loss: 0.4410817623138428 LR: 4e-05\nEpoch: 8 Batch Index: 400 Loss: 0.514415442943573 LR: 4e-05\nEpoch: 8 Batch Index: 450 Loss: 0.40804094076156616 LR: 4e-05\nEpoch: 8 Batch Index: 500 Loss: 0.4588163197040558 LR: 4e-05\nValidation Loss: 0.4598114751609026\n\nSAVING MODEL\nEnd of epoch 8\nEpoch: 9 Batch Index: 0 Loss: 0.41599738597869873 LR: 4e-05\nEpoch: 9 Batch Index: 50 Loss: 0.49865296483039856 LR: 4e-05\nEpoch: 9 Batch Index: 100 Loss: 0.4181060194969177 LR: 4e-05\nEpoch: 9 Batch Index: 150 Loss: 0.41786861419677734 LR: 4e-05\nEpoch: 9 Batch Index: 200 Loss: 0.41330641508102417 LR: 4e-05\nEpoch: 9 Batch Index: 250 Loss: 0.4484778046607971 LR: 4e-05\nEpoch: 9 Batch Index: 300 Loss: 0.4458897113800049 LR: 4e-05\nEpoch: 9 Batch Index: 350 Loss: 0.3292234241962433 LR: 4e-05\nEpoch: 9 Batch Index: 400 Loss: 0.49081942439079285 LR: 4e-05\nEpoch: 9 Batch Index: 450 Loss: 0.35482242703437805 LR: 4e-05\nEpoch: 9 Batch Index: 500 Loss: 0.46021533012390137 LR: 4e-05\nValidation Loss: 0.46454002002699185\n\nSAVING MODEL\nEnd of epoch 9\n"
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    best_val = float('inf')\n",
    "    train_epoch(model, optimizer, train_data_loader, lr_scheduler)\n",
    "    val_loss = validate_epoch(model, valid_data_loader)  \n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        print(\"SAVING MODEL\")\n",
    "        torch.save(model.state_dict(), os.path.join(MODELS_OUT_DIR, f\"best_model.pth\"))\n",
    "    print(f\"End of epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.save(model.model.state_dict(), \"test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.upload_to_kaggle(\"efficientdetd5\", \"Efficient Det D5\", new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE\n",
    "class WheatTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transforms=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = dataframe['image_id'].unique()\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "\n",
    "        return image, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = WheatTestDataset(test_df, os.path.join(DATA_DIR, \"test\"), get_test_transforms())\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(ck_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    test_model = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size = 512\n",
    "    test_model.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=0.001, momentum=0.01))\n",
    "\n",
    "    checkpoint = torch.load(ck_path)\n",
    "    test_model.load_state_dict(checkpoint)\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    test_model = DetBenchPredict(test_model, config)\n",
    "    test_model.eval()\n",
    "    return test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = load_test_model(\"test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-784042e6fd26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "detection_threshold = 0.5\n",
    "results = []\n",
    "\n",
    "for images, image_ids in test_data_loader:\n",
    "    images = torch.stack(images).to(DEVICE).float()\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model(images, torch.tensor([1]*images.shape[0]).to(DEVICE).float(), torch.tensor([512, 512]*images.shape[0]).to(DEVICE).float())\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "\n",
    "            boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "            scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "            \n",
    "            boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "            scores = scores[scores >= detection_threshold]\n",
    "            image_id = image_ids[i]\n",
    "            \n",
    "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "            \n",
    "            result = {\n",
    "                'image_id': image_id,\n",
    "                'PredictionString': format_prediction_string(boxes, scores)\n",
    "            }\n",
    "\n",
    "            \n",
    "            results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}